{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3d7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Home/siv31/fla023/master/Hubert-Final-Project/project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fb0384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv31/fla023/.conda/envs/HUBERT/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.utils.Environments import OpenAIEnv as Environment\n",
    "from src.utils import DQN, Transition, ReplayMemory, read_tsv_file\n",
    "from random import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from math import exp\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbd3630",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = read_tsv_file('../resources/map_4.dat', enemies={}, start=(46, 1), end=(1, 46))\n",
    "env = Environment(name=\"Level 5\", grid=grid, project_path='Levels')\n",
    "# env.plot(color_bar=False, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0f8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETERS\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.2\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get the number of state observations\n",
    "state = env.reset()\n",
    "\n",
    "n_observations = len(state)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "policy_net = DQN(input_dim=n_observations, output_dim=n_actions, device=device)\n",
    "target_net = DQN(input_dim=n_observations, output_dim=n_actions, device=device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ae4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1).clamp(0, 2)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3893bb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f2ca7c4ab90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4aa86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ec7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device,\n",
    "                                  dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6467c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[POLICY NET] cuda:0\n",
      "[TARGET NET] cuda:0\n",
      "[NUM EPISODES] 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[1]], device='cuda:0') 46 1\n",
      "10000 tensor([[2]], device='cuda:0') 46 46\n",
      "20000 tensor([[0]], device='cuda:0') 46 1\n",
      "30000 tensor([[0]], device='cuda:0') 46 1\n",
      "40000 tensor([[0]], device='cuda:0') 46 3\n",
      "50000 tensor([[2]], device='cuda:0') 46 45\n",
      "60000 tensor([[2]], device='cuda:0') 46 45\n",
      "70000 tensor([[2]], device='cuda:0') 46 45\n",
      "80000 tensor([[2]], device='cuda:0') 46 46\n",
      "90000 tensor([[2]], device='cuda:0') 46 1\n",
      "100000 tensor([[2]], device='cuda:0') 46 2\n",
      "110000 tensor([[2]], device='cuda:0') 46 46\n",
      "120000 tensor([[2]], device='cuda:0') 46 45\n",
      "130000 tensor([[0]], device='cuda:0') 46 2\n",
      "140000 tensor([[2]], device='cuda:0') 46 1\n",
      "150000 tensor([[2]], device='cuda:0') 46 2\n",
      "160000 tensor([[2]], device='cuda:0') 46 46\n",
      "170000 tensor([[0]], device='cuda:0') 46 1\n",
      "180000 tensor([[2]], device='cuda:0') 46 2\n",
      "190000 tensor([[0]], device='cuda:0') 46 3\n",
      "200000 tensor([[2]], device='cuda:0') 46 46\n",
      "210000 tensor([[2]], device='cuda:0') 46 45\n",
      "220000 tensor([[2]], device='cuda:0') 46 46\n",
      "230000 tensor([[0]], device='cuda:0') 46 1\n",
      "240000 tensor([[2]], device='cuda:0') 46 1\n",
      "250000 tensor([[0]], device='cuda:0') 46 2\n",
      "260000 tensor([[2]], device='cuda:0') 46 2\n",
      "270000 tensor([[0]], device='cuda:0') 46 1\n",
      "280000 tensor([[0]], device='cuda:0') 46 1\n",
      "290000 tensor([[2]], device='cuda:0') 46 46\n",
      "300000 tensor([[2]], device='cuda:0') 46 45\n",
      "310000 tensor([[0]], device='cuda:0') 46 46\n",
      "320000 tensor([[0]], device='cuda:0') 46 1\n",
      "330000 tensor([[0]], device='cuda:0') 46 1\n",
      "340000 tensor([[0]], device='cuda:0') 46 2\n",
      "350000 tensor([[0]], device='cuda:0') 46 3\n",
      "360000 tensor([[0]], device='cuda:0') 46 3\n",
      "370000 tensor([[2]], device='cuda:0') 46 45\n",
      "380000 tensor([[2]], device='cuda:0') 46 45\n",
      "390000 tensor([[2]], device='cuda:0') 46 46\n",
      "400000 tensor([[2]], device='cuda:0') 45 45\n",
      "410000 tensor([[0]], device='cuda:0') 46 1\n",
      "420000 tensor([[0]], device='cuda:0') 45 2\n",
      "430000 tensor([[0]], device='cuda:0') 46 2\n",
      "440000 tensor([[0]], device='cuda:0') 46 1\n",
      "450000 tensor([[0]], device='cuda:0') 46 1\n",
      "460000 tensor([[2]], device='cuda:0') 46 3\n",
      "470000 tensor([[2]], device='cuda:0') 46 46\n",
      "480000 tensor([[0]], device='cuda:0') 46 46\n",
      "490000 tensor([[2]], device='cuda:0') 46 46\n",
      "500000 tensor([[0]], device='cuda:0') 46 46\n",
      "510000 tensor([[2]], device='cuda:0') 46 44\n",
      "520000 tensor([[0]], device='cuda:0') 44 8\n",
      "530000 tensor([[0]], device='cuda:0') 46 1\n",
      "540000 tensor([[2]], device='cuda:0') 46 46\n",
      "550000 tensor([[2]], device='cuda:0') 46 46\n",
      "560000 tensor([[0]], device='cuda:0') 46 1\n",
      "570000 tensor([[0]], device='cuda:0') 46 2\n",
      "580000 tensor([[2]], device='cuda:0') 46 1\n",
      "590000 tensor([[2]], device='cuda:0') 46 1\n",
      "600000 tensor([[0]], device='cuda:0') 46 2\n",
      "610000 tensor([[0]], device='cuda:0') 46 2\n",
      "620000 tensor([[0]], device='cuda:0') 46 1\n",
      "630000 tensor([[0]], device='cuda:0') 46 2\n",
      "640000 tensor([[2]], device='cuda:0') 46 46\n",
      "650000 tensor([[0]], device='cuda:0') 46 46\n",
      "660000 tensor([[2]], device='cuda:0') 46 45\n",
      "670000 tensor([[0]], device='cuda:0') 46 46\n",
      "680000 tensor([[0]], device='cuda:0') 46 45\n",
      "690000 tensor([[0]], device='cuda:0') 46 1\n",
      "700000 tensor([[0]], device='cuda:0') 46 1\n",
      "710000 tensor([[0]], device='cuda:0') 46 1\n",
      "720000 tensor([[0]], device='cuda:0') 46 2\n",
      "730000 tensor([[0]], device='cuda:0') 46 1\n",
      "740000 tensor([[0]], device='cuda:0') 46 2\n",
      "750000 tensor([[0]], device='cuda:0') 46 2\n",
      "760000 tensor([[2]], device='cuda:0') 46 1\n",
      "770000 tensor([[2]], device='cuda:0') 46 1\n",
      "780000 tensor([[2]], device='cuda:0') 46 1\n",
      "790000 tensor([[1]], device='cuda:0') 46 2\n",
      "800000 tensor([[0]], device='cuda:0') 46 1\n",
      "810000 tensor([[2]], device='cuda:0') 46 1\n",
      "820000 tensor([[2]], device='cuda:0') 46 46\n",
      "830000 tensor([[2]], device='cuda:0') 46 45\n",
      "840000 tensor([[2]], device='cuda:0') 46 46\n",
      "850000 tensor([[2]], device='cuda:0') 46 45\n",
      "860000 tensor([[0]], device='cuda:0') 46 45\n",
      "870000 tensor([[2]], device='cuda:0') 46 44\n",
      "880000 tensor([[0]], device='cuda:0') 46 45\n",
      "890000 tensor([[0]], device='cuda:0') 46 1\n",
      "900000 tensor([[0]], device='cuda:0') 46 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   0%|          | 0/100000 [34:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     39\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m optimize_model()\n\u001b[1;32m     42\u001b[0m \u001b[39m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m     44\u001b[0m target_net_state_dict \u001b[39m=\u001b[39m target_net\u001b[39m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     48\u001b[0m \u001b[39m# In-place gradient clipping\u001b[39;00m\n\u001b[1;32m     49\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(policy_net\u001b[39m.\u001b[39mparameters(), \u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/HUBERT/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/HUBERT/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 100_000\n",
    "else:\n",
    "    num_episodes = 1_000\n",
    "\n",
    "print(f'[POLICY NET] {next(policy_net.parameters()).device}')\n",
    "print(f'[TARGET NET] {next(target_net.parameters()).device}')\n",
    "print(f'[NUM EPISODES] {num_episodes}')\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes), desc='Episode'):\n",
    "    # Initialize the environment and get it's state\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        if t % 10_000 == 0: print(t, action, env.y, env.x)\n",
    "        observation, reward, terminated, truncated = env.step(action.item())\n",
    "        total_reward += reward\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "            \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            # plot_durations()\n",
    "            break\n",
    "\n",
    "    print(total_reward)\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597fdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
